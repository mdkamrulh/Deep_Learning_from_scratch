{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cW39leNSGpl3"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "#print(os.listdir(\"../input\"))\n",
        "\n",
        "import time\n",
        "\n",
        "# import pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD,Adam,lr_scheduler\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y2wBEaNyGpl6"
      },
      "outputs": [],
      "source": [
        "# define transformations for train\n",
        "train_transform = transforms.Compose([\n",
        "    #transforms.RandomHorizontalFlip(p=.40),\n",
        "   # transforms.RandomRotation(30),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "\n",
        "# define transformations for test\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "\n",
        "# define training dataloader\n",
        "def get_training_dataloader(train_transform, batch_size=128, num_workers=0, shuffle=True):\n",
        "    \"\"\" return training dataloader\n",
        "    Args:\n",
        "        train_transform: transfroms for train dataset\n",
        "        path: path to cifar100 training python dataset\n",
        "        batch_size: dataloader batchsize\n",
        "        num_workers: dataloader num_works\n",
        "        shuffle: whether to shuffle \n",
        "    Returns: train_data_loader:torch dataloader object\n",
        "    \"\"\"\n",
        "\n",
        "    transform_train = train_transform\n",
        "    cifar10_training = torchvision.datasets.CIFAR10(root='.', train=True, download=True, transform=transform_train)\n",
        "    cifar10_training_loader = DataLoader(\n",
        "        cifar10_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
        "\n",
        "    return cifar10_training_loader\n",
        "\n",
        "# define test dataloader\n",
        "def get_testing_dataloader(test_transform, batch_size=128, num_workers=0, shuffle=True):\n",
        "    \"\"\" return training dataloader\n",
        "    Args:\n",
        "        test_transform: transforms for test dataset\n",
        "        path: path to cifar100 test python dataset\n",
        "        batch_size: dataloader batchsize\n",
        "        num_workers: dataloader num_works\n",
        "        shuffle: whether to shuffle \n",
        "    Returns: cifar100_test_loader:torch dataloader object\n",
        "    \"\"\"\n",
        "\n",
        "    transform_test = test_transform\n",
        "    cifar10_test = torchvision.datasets.CIFAR10(root='.', train=False, download=True, transform=transform_test)\n",
        "    cifar10_test_loader = DataLoader(\n",
        "        cifar10_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
        "\n",
        "    return cifar10_test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UaBLIrnLGpl7"
      },
      "outputs": [],
      "source": [
        "# implement mish activation function\n",
        "def f_mish(input):\n",
        "    '''\n",
        "    Applies the mish function element-wise:\n",
        "    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
        "    '''\n",
        "    return input * torch.tanh(F.softplus(input))\n",
        "\n",
        "# implement class wrapper for mish activation function\n",
        "class mish(nn.Module):\n",
        "    '''\n",
        "    Applies the mish function element-wise:\n",
        "    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
        "\n",
        "    Shape:\n",
        "        - Input: (N, *) where * means, any number of additional\n",
        "          dimensions\n",
        "        - Output: (N, *), same shape as the input\n",
        "\n",
        "    Examples:\n",
        "        >>> m = mish()\n",
        "        >>> input = torch.randn(2)\n",
        "        >>> output = m(input)\n",
        "\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Init method.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Forward pass of the function.\n",
        "        '''\n",
        "        return f_mish(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TNYYtN-0Gpl7"
      },
      "outputs": [],
      "source": [
        "def soft_threshold(input, lower=-0.05, upper =0.05):\n",
        "    values_below_lower = torch.where(input<lower, inputs-lower,torch.zeros(1))\n",
        "    values_above_upper = torch.where(upper<input, inputs-upper,torch.zeros(1))\n",
        "    output= values_below_lower+values_above_upper\n",
        "    return output\n",
        "\n",
        "class Soft_Threshold(nn.Module):\n",
        "    def __init__(self, lower=-0.05, upper=0.05):\n",
        "        super(Soft_Threshold, self).__init__()\n",
        "        self.lower = lower\n",
        "        self.upper = upper\n",
        "        \n",
        "    def forward(self, input):\n",
        "        return soft_threshold(input, self.lower, self.upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4tx0VxNIGpl8"
      },
      "outputs": [],
      "source": [
        "# implement swish activation function\n",
        "def f_swish(input):\n",
        "    '''\n",
        "    Applies the swish function element-wise:\n",
        "    swish(x) = x * sigmoid(x)\n",
        "    '''\n",
        "    return input * torch.sigmoid(input)\n",
        "\n",
        "# implement class wrapper for swish activation function\n",
        "class swish(nn.Module):\n",
        "    '''\n",
        "    Applies the swish function element-wise:\n",
        "    swish(x) = x * sigmoid(x)\n",
        "\n",
        "    Shape:\n",
        "        - Input: (N, *) where * means, any number of additional\n",
        "          dimensions\n",
        "        - Output: (N, *), same shape as the input\n",
        "\n",
        "    Examples:\n",
        "        >>> m = swish()\n",
        "        >>> input = torch.randn(2)\n",
        "        >>> output = m(input)\n",
        "\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Init method.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Forward pass of the function.\n",
        "        '''\n",
        "        return f_swish(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xcQrC-UZGpl8"
      },
      "outputs": [],
      "source": [
        "#\"\"\"Bottleneck layers. Although each layer only produces k\n",
        "#output feature-maps, it typically has many more inputs. It\n",
        "#has been noted in [37, 11] that a 1×1 convolution can be in-\n",
        "#troduced as bottleneck layer before each 3×3 convolution\n",
        "#to reduce the number of input feature-maps, and thus to\n",
        "#improve computational efficiency.\"\"\"\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, activation = 'relu'):\n",
        "        super().__init__()\n",
        "        #\"\"\"In  our experiments, we let each 1×1 convolution \n",
        "        #produce 4k feature-maps.\"\"\"\n",
        "        inner_channel = 4 * growth_rate\n",
        "        \n",
        "        if activation == 'relu':\n",
        "            f_activation = nn.ReLU(inplace=True)\n",
        "            \n",
        "        if activation == 'swish':\n",
        "            f_activation = swish()\n",
        "            \n",
        "        if activation == 'mish':\n",
        "            f_activation = mish()\n",
        "\n",
        "        if activation == 'soft_threshold':\n",
        "            f_activation = nn.Softshrink(lambd= 0.00005)\n",
        "\n",
        "        #\"\"\"We find this design especially effective for DenseNet and \n",
        "        #we refer to our network with such a bottleneck layer, i.e., \n",
        "        #to the BN-ReLU-Conv(1×1)-BN-ReLU-Conv(3×3) version of H ` , \n",
        "        #as DenseNet-B.\"\"\"\n",
        "        self.bottle_neck = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            f_activation,\n",
        "            nn.Conv2d(in_channels, inner_channel, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(inner_channel),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(inner_channel, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([x, self.bottle_neck(x)], 1)\n",
        "\n",
        "#\"\"\"We refer to layers between blocks as transition\n",
        "#layers, which do convolution and pooling.\"\"\"\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        #\"\"\"The transition layers used in our experiments \n",
        "        #consist of a batch normalization layer and an 1×1 \n",
        "        #convolutional layer followed by a 2×2 average pooling \n",
        "        #layer\"\"\".\n",
        "        self.down_sample = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
        "            nn.AvgPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_sample(x)\n",
        "\n",
        "#DesneNet-BC\n",
        "#B stands for bottleneck layer(BN-RELU-CONV(1x1)-BN-RELU-CONV(3x3))\n",
        "#C stands for compression factor(0<=theta<=1)\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_class=10, activation = 'relu'):\n",
        "        super().__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "\n",
        "        #\"\"\"Before entering the first dense block, a convolution \n",
        "        #with 16 (or twice the growth rate for DenseNet-BC) \n",
        "        #output channels is performed on the input images.\"\"\"\n",
        "        inner_channels = 2 * growth_rate\n",
        "\n",
        "        #For convolutional layers with kernel size 3×3, each \n",
        "        #side of the inputs is zero-padded by one pixel to keep \n",
        "        #the feature-map size fixed.\n",
        "        self.conv1 = nn.Conv2d(3, inner_channels, kernel_size=3, padding=1, bias=False) \n",
        "        \n",
        "        if activation == 'relu':\n",
        "            f_activation = nn.ReLU(inplace=True)\n",
        "            \n",
        "        if activation == 'swish':\n",
        "            f_activation = swish()\n",
        "            \n",
        "        if activation == 'mish':\n",
        "            f_activation = mish()\n",
        "        \n",
        "        if activation == 'soft_threshold':\n",
        "            f_activation = nn.Softshrink(lambd=0.00005)\n",
        "\n",
        "        self.features = nn.Sequential()\n",
        "\n",
        "        for index in range(len(nblocks) - 1):\n",
        "            self.features.add_module(\"dense_block_layer_{}\".format(index), self._make_dense_layers(block, inner_channels, nblocks[index]))\n",
        "            inner_channels += growth_rate * nblocks[index]\n",
        "\n",
        "            #\"\"\"If a dense block contains m feature-maps, we let the \n",
        "            #following transition layer generate θm output feature-\n",
        "            #maps, where 0 < θ ≤ 1 is referred to as the compression \n",
        "            #fac-tor.\n",
        "            out_channels = int(reduction * inner_channels) # int() will automatic floor the value\n",
        "            self.features.add_module(\"transition_layer_{}\".format(index), Transition(inner_channels, out_channels))\n",
        "            inner_channels = out_channels\n",
        "\n",
        "        self.features.add_module(\"dense_block{}\".format(len(nblocks) - 1), self._make_dense_layers(block, inner_channels, nblocks[len(nblocks)-1]))\n",
        "        inner_channels += growth_rate * nblocks[len(nblocks) - 1]\n",
        "        self.features.add_module('bn', nn.BatchNorm2d(inner_channels))\n",
        "        self.features.add_module('activation', f_activation)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.linear = nn.Linear(inner_channels, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "       # print(output.shape)\n",
        "        output = self.features(output)\n",
        "        output = self.avgpool(output)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "    def _make_dense_layers(self, block, in_channels, nblocks):\n",
        "        dense_block = nn.Sequential()\n",
        "        for index in range(nblocks):\n",
        "            dense_block.add_module('bottle_neck_layer_{}'.format(index), block(in_channels, self.growth_rate))\n",
        "            in_channels += self.growth_rate\n",
        "        return dense_block\n",
        "\n",
        "def densenet121(activation = 'relu'):\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32, activation = activation)\n",
        "\n",
        "def densenet169(activation = 'relu'):\n",
        "    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32, activation = activation)\n",
        "\n",
        "def densenet201(activation = 'relu'):\n",
        "    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32, activation = activation)\n",
        "\n",
        "def densenet161(activation = 'relu'):\n",
        "    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48, activation = activation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTVOrFXbGpl9",
        "outputId": "1f22b07d-bfef-4438-8d74-42eda2467f77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: .\\cifar-10-python.tar.gz\n",
            "Extracting .\\cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainloader = get_training_dataloader(train_transform)\n",
        "testloader = get_testing_dataloader(test_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALrExYS-Gpl-",
        "outputId": "a462dd15-38e4-4e90-c1bf-5fd167e73f3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs = 50\n",
        "batch_size = 128\n",
        "learning_rate = 0.0001\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hJdMMKpvGpl_"
      },
      "outputs": [],
      "source": [
        "def modified_one_hot(labels, num_classes=10):\n",
        "  one_hot = F.one_hot(labels, num_classes=10)\n",
        "  one_hot[one_hot==0]=-1.\n",
        "  one_hot[one_hot==1]=1.\n",
        "  return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KTy7I4rnGpl_"
      },
      "outputs": [],
      "source": [
        "def Exponential_loss(predictions, targets):\n",
        "    one_hot_values= modified_one_hot(targets)\n",
        "    total_loss = torch.exp(-(torch.mul(predictions, one_hot_values)))\n",
        "    return total_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l7tCwAD5GpmA"
      },
      "outputs": [],
      "source": [
        "model = densenet169(activation = 'soft_threshold')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dWKPNHjXGpmA"
      },
      "outputs": [],
      "source": [
        "# set loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# set optimizer, only train the classifier parameters, feature parameters are frozen\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "coK-7ABKGpmA"
      },
      "outputs": [],
      "source": [
        "train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy', 'Train top-3 accuracy','Test loss', 'Test accuracy', 'Test top-3 accuracy']) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzMrwJm2GpmB",
        "outputId": "c21e1199-d54b-49f1-b278-6570a8d0d460"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Kamrul\\Desktop\\DenseNet169_Soft_theshold.ipynb Cell 14'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kamrul/Desktop/DenseNet169_Soft_theshold.ipynb#ch0000013?line=18'>19</a>\u001b[0m logps \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kamrul/Desktop/DenseNet169_Soft_theshold.ipynb#ch0000013?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m Exponential_loss(logps, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kamrul/Desktop/DenseNet169_Soft_theshold.ipynb#ch0000013?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kamrul/Desktop/DenseNet169_Soft_theshold.ipynb#ch0000013?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kamrul/Desktop/DenseNet169_Soft_theshold.ipynb#ch0000013?line=23'>24</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Kamrul/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#train the model\n",
        "model.to(device)\n",
        "\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    since = time.time()\n",
        "    \n",
        "    train_accuracy = 0\n",
        "    top3_train_accuracy = 0 \n",
        "    for inputs, labels in trainloader:\n",
        "        steps += 1\n",
        "        # Move input and label tensors to the default device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logps = model.forward(inputs)\n",
        "        loss = Exponential_loss(logps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        # calculate train top-1 accuracy\n",
        "        ps = torch.exp(logps)\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "        \n",
        "        # Calculate train top-3 accuracy\n",
        "        np_top3_class = ps.topk(3, dim=1)[1].cpu().numpy()\n",
        "        target_numpy = labels.cpu().numpy()\n",
        "        top3_train_accuracy += np.mean([1 if target_numpy[i] in np_top3_class[i] else 0 for i in range(0, len(target_numpy))])\n",
        "        \n",
        "    time_elapsed = time.time() - since\n",
        "    \n",
        "    test_loss = 0\n",
        "    test_accuracy = 0\n",
        "    top3_test_accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            logps = model.forward(inputs)\n",
        "            batch_loss = Exponential_loss(logps, labels)\n",
        "            test_loss += batch_loss.item()\n",
        "\n",
        "            # Calculate test top-1 accuracy\n",
        "            ps = torch.exp(logps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "            \n",
        "            # Calculate test top-3 accuracy\n",
        "            np_top3_class = ps.topk(3, dim=1)[1].cpu().numpy()\n",
        "            target_numpy = labels.cpu().numpy()\n",
        "            top3_test_accuracy += np.mean([1 if target_numpy[i] in np_top3_class[i] else 0 for i in range(0, len(target_numpy))])\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "          f\"Time per epoch: {time_elapsed:.4f}.. \"\n",
        "          f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n",
        "          f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n",
        "          f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n",
        "          f\"Top-3 train accuracy: {top3_train_accuracy/len(trainloader):.4f}.. \"\n",
        "          f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n",
        "          f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \"\n",
        "          f\"Top-3 test accuracy: {top3_test_accuracy/len(testloader):.4f}\")\n",
        "\n",
        "    train_stats = train_stats.append({'Epoch': epoch, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader), 'Train accuracy': train_accuracy/len(trainloader), 'Train top-3 accuracy':top3_train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader), 'Test accuracy': test_accuracy/len(testloader), 'Test top-3 accuracy':top3_test_accuracy/len(testloader)}, ignore_index=True)\n",
        "\n",
        "    running_loss = 0\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dheBmQukGpmB"
      },
      "outputs": [],
      "source": [
        "inputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2uVwuFqGpmB"
      },
      "outputs": [],
      "source": [
        " train_stats.to_csv('train_log_InceptionV3_Soft_thresholdth0.00005-0.0003.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRdvOp5QGpmC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DenseNet169-Soft_theshold.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "adc8c5e3a5c2698cbab5dd206cb732d3e63acbde744b69bc42c3c564d0b0f573"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
